"""
Tobit Model
"""
# pylint: disable=C0103
from functools import partial
from typing import List, Optional

from jax import grad, hessian, jit
from jax.numpy import DeviceArray
import jax.numpy as jnp
from jax.scipy.stats.norm import logcdf, logpdf
from numpy.typing import ArrayLike
from pandas import DataFrame
from regmod.data import Data
from regmod.function import SmoothFunction

from .model import Model


identity_jax = SmoothFunction(
    name="identity_jax",
    fun=jnp.array,
    inv_fun=jnp.array,
    dfun=jnp.ones_like,
    d2fun=jnp.zeros_like
)


exp_jax = SmoothFunction(
    name="exp_jax",
    fun=jnp.exp,
    inv_fun=jnp.log,
    dfun=jnp.exp,
    d2fun=jnp.exp
)


class TobitModel(Model):
    """Tobit model class.

    A tobit regression model is used for data generated by a Gaussian
    distribution with all negative elements set to 0 (i.e., a censored
    or rectified Gaussian distribution).

    """

    param_names = ("mu", "sigma")
    default_param_specs = {
        "mu": {"inv_link": identity_jax},
        "sigma": {"inv_link": exp_jax}
    }

    def __init__(self, data: Data, **kwargs) -> None:
        """Initialize tobit model.

        Parameters
        ----------
        data : Data
            Training data.

        Raises
        ------
        ValueError
            If negative observations present in `data`.

        Notes
        -----
        User input for parameter attribute `inv_link` is ingored and
        defaults used instead.

        """
        if not jnp.all(data.obs >= 0):
            raise ValueError("Tobit model requires non-negative observations.")
        super().__init__(data, **kwargs)

        # Use JAX inv_link functions
        for ii, param_name in enumerate(self.param_names):
            default_link = self.default_param_specs[param_name]["inv_link"]
            self.params[ii].inv_link = default_link

        # Use JAX data structures
        self.mat = [jnp.asarray(mat) for mat in self.mat]
        self.uvec = jnp.asarray(self.uvec)
        self.gvec = jnp.asarray(self.gvec)
        self.linear_uvec = jnp.asarray(self.linear_uvec)
        self.linear_gvec = jnp.asarray(self.linear_gvec)
        self.linear_umat = jnp.asarray(self.linear_umat)
        self.linear_gmat = jnp.asarray(self.linear_gmat)

    @partial(jit, static_argnums=(0,))
    def objective(self, coefs: ArrayLike) -> float:
        """Get negative log likelihood wrt coefficients.

        Parameters
        ----------
        coefs : array_like
            Model coefficients.

        Returns
        -------
        float
            Negative log likelihood.

        """
        # self.data.weights included in get_nll_terms
        obj_param = self.data.trim_weights*self.get_nll_terms(coefs)
        return obj_param.sum() + self.objective_from_gprior(coefs)

    @partial(jit, static_argnums=(0,))
    def gradient(self, coefs: ArrayLike) -> DeviceArray:
        """Get gradient of negative log likelihood wrt coefficients.

        Parameters
        ----------
        coefs : array_like
            Model coefficients.

        Returns
        -------
        DeviceArray
            Gradient of negative log likelihood.

        """
        return grad(self.objective)(coefs)

    @partial(jit, static_argnums=(0,))
    def hessian(self, coefs: ArrayLike) -> DeviceArray:
        """Get hessian of negative log likelihood wrt coefficients.

        Parameters
        ----------
        coefs : array_like
            Model coefficients.

        Returns
        -------
        DeviceArray
            Hessian of negative log likelihood.

        """
        return hessian(self.objective)(coefs)

    @partial(jit, static_argnums=(0,))
    def nll(self, params: List[ArrayLike]) -> DeviceArray:
        """Get terms of negative log likelihood wrt parameters.

        Parameters
        ----------
        params : list[array_like]
            Model parameters.

        Returns
        -------
        DeviceArray
            Terms of negative log likelihood.

        """
        y = self.data.obs
        mu = params[0]
        sigma = params[1]
        pos_term = jnp.log(sigma) - logpdf((y - mu)/sigma)
        npos_term = -logcdf(-mu/sigma)
        return jnp.where(y > 0, pos_term, npos_term)

    @partial(jit, static_argnums=(0,))
    def dnll(self, params: List[ArrayLike]) -> List[DeviceArray]:
        """Get derivative of negative log likelihood wrt parameters.

        Parameters
        ----------
        params : list[array_like]
            Model parameters.

        Returns
        -------
        list[DeviceArray]
            Derivatives of negative log likelihood.

        """
        return grad(lambda pars: jnp.sum(self.nll(pars)))(params)

    def get_vcov(self, coefs: ArrayLike) -> DeviceArray:
        """Get variance-covariance matrix.

        Parameters
        ----------
        coefs : array_like
            Model coefficients.

        Returns
        -------
        DeviceArray
            Variance-covariance matrix.

        """
        H = self.hessian(coefs)
        J = self.jacobian2(coefs)
        inv_H = jnp.linalg.inv(H)
        return inv_H.dot(J.dot(inv_H.T))

    def predict(self, df: Optional[DataFrame] = None) -> DataFrame:
        """Predict mu, sigma, and censored mu.

        Parameters
        ----------
        df : DataFrame, optional
            Prediction data. If None, use training data.

        Returns
        -------
        DataFrame
            Data frame with predicted parameters.

        """
        df = super().predict(df)
        mu = jnp.asarray(df["mu"])
        df["mu_censored"] = jnp.where(mu > 0, mu, 0)
        return df
