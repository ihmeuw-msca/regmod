"""
Tobit Model
"""
# pylint: disable=C0103
from functools import partial
from typing import Optional

from jax import grad, hessian, jit, lax
from jax.numpy import DeviceArray
import jax.numpy as jnp
from jax.scipy.stats.norm import logcdf, logpdf
from numpy.typing import ArrayLike
from pandas import DataFrame
from regmod.data import Data
from regmod.function import SmoothFunction

from .model import Model


identity_jax = SmoothFunction(
    name="identity_jax",
    fun=jnp.array,
    inv_fun=jnp.array,
    dfun=jnp.ones_like,
    d2fun=jnp.zeros_like
)


exp_jax = SmoothFunction(
    name="exp_jax",
    fun=jnp.exp,
    inv_fun=jnp.log,
    dfun=jnp.exp,
    d2fun=jnp.exp
)


class TobitModel(Model):
    """Tobit model class.

    A tobit regression model is used for data generated by a Gaussian
    distribution with all negative elements set to 0 (i.e., a censored
    or rectified Gaussian distribution).

    """

    param_names = ("mu", "sigma")
    default_param_specs = {
        "mu": {"inv_link": identity_jax},
        "sigma": {"inv_link": exp_jax}
    }

    def __init__(self, data: Data, **kwargs) -> None:
        """Initialize tobit model.

        Parameters
        ----------
        data : Data
            Training data.

        Raises
        ------
        ValueError
            If negative observations present in `data`.

        Notes
        -----
        User input for parameter attribute `inv_link` is ingored and
        defaults used instead.

        """
        if not jnp.all(data.obs >= 0):
            raise ValueError("Tobit model requires non-negative observations.")
        super().__init__(data, **kwargs)

        # Use JAX inv_link functions
        for ii, param_name in enumerate(self.param_names):
            default_link = self.default_param_specs[param_name]["inv_link"]
            self.params[ii].inv_link = default_link

    def get_mat(self) -> list[DeviceArray]:
        """Get the design matrices.

        Returns
        -------
        list[DeviceArray]
            The design matrices.

        """
        return [jnp.asarray(mat) for mat in super().get_mat()]

    @partial(jit, static_argnums=(0,))
    def objective(self, coefs: ArrayLike) -> float:
        """Get negative log likelihood wrt coefficients.

        Parameters
        ----------
        coefs : array_like
            Model coefficients.

        Returns
        -------
        float
            Negative log likelihood.

        """
        weights = self.data.weights*self.data.trim_weights
        obj_param = weights*self.get_nll_terms(coefs)
        return obj_param.sum() + self.objective_from_gprior(coefs)

    @partial(jit, static_argnums=(0,))
    def gradient(self, coefs: ArrayLike) -> DeviceArray:
        """Get gradient of negative log likelihood wrt coefficients.

        Parameters
        ----------
        coefs : array_like
            Model coefficients.

        Returns
        -------
        DeviceArray
            Gradient of negative log likelihood.

        """
        return grad(self.objective)(coefs)

    @partial(jit, static_argnums=(0,))
    def hessian(self, coefs: ArrayLike) -> DeviceArray:
        """Get hessian of negative log likelihood wrt coefficients.

        Parameters
        ----------
        coefs : array_like
            Model coefficients.

        Returns
        -------
        DeviceArray
            Hessian of negative log likelihood.

        """
        return hessian(self.objective)(coefs)

    @partial(jit, static_argnums=(0,))
    def nll(self, params: list[ArrayLike]) -> DeviceArray:
        """Get terms of negative log likelihood wrt parameters.

        Parameters
        ----------
        params : list[array_like]
            Model parameters.

        Returns
        -------
        DeviceArray
            Terms of negative log likelihood.

        """
        vals = {
            "mu": params[0],
            "sigma": params[1],
            "y": self.data.obs,
            "nll_terms": jnp.zeros(self.data.num_obs)
        }
        vals = lax.fori_loop(0, self.data.num_obs, _nll_term, vals)
        return vals["nll_terms"]

    @partial(jit, static_argnums=(0,))
    def dnll(self, params: list[ArrayLike]) -> list[DeviceArray]:
        """Get derivative of negative log likelihood wrt parameters.

        Parameters
        ----------
        params : list[array_like]
            Model parameters.

        Returns
        -------
        list[DeviceArray]
            Derivatives of negative log likelihood.

        """
        return grad(lambda pars: jnp.sum(self.nll(pars)))(params)

    def get_vcov(self, coefs: ArrayLike) -> DeviceArray:
        """Get variance-covariance matrix.

        Parameters
        ----------
        coefs : array_like
            Model coefficients.

        Returns
        -------
        DeviceArray
            Variance-covariance matrix.

        """
        H = self.hessian(coefs)
        J = self.jacobian2(coefs)
        inv_H = jnp.linalg.inv(H)
        return inv_H.dot(J.dot(inv_H.T))

    def predict(self, df: Optional[DataFrame] = None) -> DataFrame:
        """Predict mu and censored mu.

        Parameters
        ----------
        df : DataFrame, optional
            Prediction data. If None, use training data.

        Returns
        -------
        DataFrame
            Data frame with predicted parameters.

        """
        df = super().predict(df)
        mu = jnp.asarray(df["mu"])
        df["mu_censored"] = jnp.where(mu > 0, mu, 0)
        return df


def _nll_term(ii: int, vals: dict) -> dict:
    """Get negative log likelihood term for y[ii]."""
    term = lax.cond(vals["y"][ii] > 0, _pos_term, _npos_term, *(ii, vals))
    vals["nll_terms"] = vals["nll_terms"].at[ii].set(term)
    return vals


def _pos_term(ii: int, vals: dict) -> float:
    """Get negative log likelihood term for y[ii] > 0."""
    inner = (vals["y"][ii] - vals["mu"][ii])/vals["sigma"][ii]
    return jnp.log(vals["sigma"][ii]) - logpdf(inner)


def _npos_term(ii: int, vals: dict) -> float:
    """Get negative log likelihood term for y[ii] <= 0."""
    return -logcdf(-vals["mu"][ii]/vals["sigma"][ii])
